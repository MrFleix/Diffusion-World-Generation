{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, image_dir, patch_size=100, transform=None):\n",
    "        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform\n",
    "        self.patches = []\n",
    "\n",
    "        # Extrahiere alle Patches beim Init\n",
    "        for path in self.image_paths:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            w, h = img.size\n",
    "            for i in range(0, h, patch_size):\n",
    "                for j in range(0, w, patch_size):\n",
    "                    if i+patch_size <= h and j+patch_size <= w:\n",
    "                        patch = img.crop((j, i, j+patch_size, i+patch_size))\n",
    "                        self.patches.append(patch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.patches[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),             # [0,1]\n",
    "    transforms.Lambda(lambda x: x * 2 - 1)  # [-1,1]\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = PatchDataset(\"data/aerialimagelabeling/train/104_sat.jpg\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Transformationen: Tensor + [-1, 1] Normalisierung\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x * 2 - 1)\n",
    "])\n",
    "\n",
    "# MNIST Dataset laden (Training)\n",
    "dataset = MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Testweise ein Batch anzeigen\n",
    "for batch in dataloader:\n",
    "    imgs, labels = batch  # imgs: [64, 1, 28, 28], labels: [64]\n",
    "    print(imgs.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 1e-4\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "timesteps = 1000\n",
    "betas = linear_beta_schedule(timesteps)\n",
    "\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)\n",
    "\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "    sqrt_alpha = sqrt_alphas_cumprod[t][:, None, None, None]\n",
    "    sqrt_one_minus_alpha = sqrt_one_minus_alphas_cumprod[t][:, None, None, None]\n",
    "    return sqrt_alpha * x_start + sqrt_one_minus_alpha * noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        return torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "\n",
    "def conv_block(in_c, out_c):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_c, out_c, 3, padding=1),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, base_channels=64, time_emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv1 = conv_block(in_channels, base_channels)\n",
    "        self.conv2 = conv_block(base_channels, base_channels * 2)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2)\n",
    "        self.out_conv = nn.Conv2d(base_channels, in_channels, 1)\n",
    "\n",
    "        self.time_linear1 = nn.Linear(time_emb_dim, base_channels)\n",
    "        self.time_linear2 = nn.Linear(time_emb_dim, base_channels * 2)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_mlp(t)  # [B, time_emb_dim]\n",
    "        t1 = self.time_linear1(t_emb).unsqueeze(-1).unsqueeze(-1)  # [B, C1, 1, 1]\n",
    "        t2 = self.time_linear2(t_emb).unsqueeze(-1).unsqueeze(-1)  # [B, C2, 1, 1]\n",
    "\n",
    "        x1 = self.conv1(x) + t1  # [B, C1, H, W]\n",
    "        x2 = self.conv2(self.pool(x1)) + t2  # [B, C2, H/2, W/2]\n",
    "        x3 = self.up(x2)  # [B, C1, H, W]\n",
    "        x_out = self.out_conv(x3 + x1)  # Skip Connection\n",
    "        return x_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 0.0604\n",
      "Epoch 2: loss = 0.0715\n",
      "Epoch 3: loss = 0.0491\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = UNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x, _ in dataloader:\n",
    "        x = x.to(device)\n",
    "        t = torch.randint(0, timesteps, (x.size(0),), device=device).long()\n",
    "        noise = torch.randn_like(x)\n",
    "        x_noisy = q_sample(x, t, noise)\n",
    "        predicted_noise = model(x_noisy, t.float())\n",
    "\n",
    "        loss = loss_fn(predicted_noise, noise)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, img_size=28, n=1):\n",
    "    model.eval()\n",
    "    x = torch.randn((n, 1, img_size, img_size)).to(device)\n",
    "\n",
    "    for t in reversed(range(timesteps)):\n",
    "        t_batch = torch.full((n,), t, device=device, dtype=torch.float)\n",
    "        pred_noise = model(x, t_batch)\n",
    "        alpha = alphas[t]\n",
    "        alpha_hat = alphas_cumprod[t]\n",
    "        beta = betas[t]\n",
    "\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "        else:\n",
    "            noise = 0\n",
    "\n",
    "        x = (1 / torch.sqrt(alpha)) * (x - ((1 - alpha) / torch.sqrt(1 - alpha_hat)) * pred_noise) + torch.sqrt(beta) * noise\n",
    "\n",
    "    return x\n",
    "\n",
    "# Samplen und anzeigen\n",
    "samples = sample(model)\n",
    "samples = (samples.clamp(-1, 1) + 1) / 2  # zurück auf [0,1]\n",
    "\n",
    "for i in range(samples.size(0)):\n",
    "    img = samples[i].permute(1, 2, 0).cpu().numpy()\n",
    "    plt.imshow(img, cmap=\"gray\")  # ← erzwingt Graustufenanzeige\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
