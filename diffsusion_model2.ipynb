{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nfrom PIL import Image\\nimport torch\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom torchvision import transforms\\nimport numpy as np\\n\\n\\n\\nclass PatchDataset(Dataset):\\n    def __init__(self, image_dir, patch_size=100, transform=None):\\n        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\\n        self.patch_size = patch_size\\n        self.transform = transform\\n        self.patches = []\\n\\n        # Extrahiere alle Patches beim Init\\n        for path in self.image_paths:\\n            img = Image.open(path).convert(\"RGB\")\\n            w, h = img.size\\n            for i in range(0, h, patch_size):\\n                for j in range(0, w, patch_size):\\n                    if i+patch_size <= h and j+patch_size <= w:\\n                        patch = img.crop((j, i, j+patch_size, i+patch_size))\\n                        self.patches.append(patch)\\n\\n    def __len__(self):\\n        return len(self.patches)\\n\\n    def __getitem__(self, idx):\\n        img = self.patches[idx]\\n        if self.transform:\\n            img = self.transform(img)\\n        return img\\n\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),             # [0,1]\\n    transforms.Lambda(lambda x: x * 2 - 1)  # [-1,1]\\n])\\n\\n\\n\\n\\ndataset = PatchDataset(\"data/aerialimagelabeling/train/104_sat.jpg\", transform=transform)\\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, image_dir, patch_size=100, transform=None):\n",
    "        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform\n",
    "        self.patches = []\n",
    "\n",
    "        # Extrahiere alle Patches beim Init\n",
    "        for path in self.image_paths:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            w, h = img.size\n",
    "            for i in range(0, h, patch_size):\n",
    "                for j in range(0, w, patch_size):\n",
    "                    if i+patch_size <= h and j+patch_size <= w:\n",
    "                        patch = img.crop((j, i, j+patch_size, i+patch_size))\n",
    "                        self.patches.append(patch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.patches[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),             # [0,1]\n",
    "    transforms.Lambda(lambda x: x * 2 - 1)  # [-1,1]\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = PatchDataset(\"data/aerialimagelabeling/train/104_sat.jpg\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "\n",
    "# Transformationen: Tensor + [-1, 1] Normalisierung\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x * 2 - 1)\n",
    "])\n",
    "\n",
    "# MNIST Dataset laden (Training)\n",
    "dataset = FashionMNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Testweise ein Batch anzeigen\n",
    "for batch in dataloader:\n",
    "    imgs, labels = batch  # imgs: [64, 1, 28, 28], labels: [64]\n",
    "    print(imgs.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "\n",
    "# Transformationen: Tensor + [-1, 1] Normalisierung\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x * 2 - 1)\n",
    "])\n",
    "\n",
    "dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 1e-4\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "timesteps = 1000\n",
    "betas = linear_beta_schedule(timesteps)\n",
    "\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)\n",
    "\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "    sqrt_alpha = sqrt_alphas_cumprod[t][:, None, None, None]\n",
    "    sqrt_one_minus_alpha = sqrt_one_minus_alphas_cumprod[t][:, None, None, None]\n",
    "    return sqrt_alpha * x_start + sqrt_one_minus_alpha * noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: Zeitembedding\n",
    "B, time_emb_dim = 1, 128\n",
    "C1, H, W = 64, 28, 28  # Beispielwerte\n",
    "\n",
    "t = torch.tensor([10.0])  # Zeitschritt\n",
    "time_mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, time_emb_dim)\n",
    ")\n",
    "\n",
    "time_linear1 = torch.nn.Linear(time_emb_dim, C1)\n",
    "\n",
    "# Featuremap simulieren\n",
    "x1 = torch.randn((B, C1, H, W))\n",
    "\n",
    "# Time Embedding generieren\n",
    "t_emb = time_mlp(t.unsqueeze(0))        # [1, 128]\n",
    "t1 = time_linear1(t_emb).unsqueeze(-1).unsqueeze(-1)  # [1, 64, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        return torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "\n",
    "def conv_block(in_c, out_c):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_c, out_c, 3, padding=1),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_channels=64, time_emb_dim=256):\n",
    "        super(UNet, self).__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv1 = conv_block(in_channels, base_channels)\n",
    "        self.conv2 = conv_block(base_channels, base_channels * 2)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2)\n",
    "        self.out_conv = nn.Conv2d(base_channels, in_channels, 1)\n",
    "\n",
    "        self.time_linear1 = nn.Linear(time_emb_dim, base_channels)\n",
    "        self.time_linear2 = nn.Linear(time_emb_dim, base_channels * 2)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_mlp(t)  # [B, time_emb_dim]\n",
    "        t1 = self.time_linear1(t_emb).unsqueeze(-1).unsqueeze(-1)  # [B, C1, 1, 1]\n",
    "        t2 = self.time_linear2(t_emb).unsqueeze(-1).unsqueeze(-1)  # [B, C2, 1, 1]\n",
    "\n",
    "        x1 = self.conv1(x) + t1  # [B, C1, H, W]\n",
    "        x2 = self.conv2(self.pool(x1)) + t2  # [B, C2, H/2, W/2]\n",
    "        x3 = self.up(x2)  # [B, C1, H, W]\n",
    "        x_out = self.out_conv(x3 + x1)  # Skip Connection\n",
    "        return x_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 0.1138\n",
      "Epoch 2: loss = 0.1059\n",
      "Epoch 3: loss = 0.0879\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = UNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x, _ in dataloader:\n",
    "        x = x.to(device)\n",
    "        t = torch.randint(0, timesteps, (x.size(0),), device=device).long()\n",
    "        noise = torch.randn_like(x)\n",
    "        x_noisy = q_sample(x, t, noise)\n",
    "        predicted_noise = model(x_noisy, t.float())\n",
    "\n",
    "        loss = loss_fn(predicted_noise, noise)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def sample(model, img_size=32, n=1, timesteps=1000):\n",
    "    device = next(model.parameters()).device  # Das Gerät des Modells ermitteln\n",
    "    model.eval()  # Setze das Modell in den Evaluierungsmodus\n",
    "    x = torch.randn((n, 3, img_size, img_size), device=device)  # Starte mit Rauschen (3 Kanäle für RGB)\n",
    "\n",
    "    # Beispielwerte für alpha, alpha_hat und beta (müssen angepasst werden)\n",
    "    alpha = torch.tensor(0.9, device=device)  # Umwandlung in Tensor\n",
    "    alpha_hat = torch.tensor(0.8, device=device)  # Umwandlung in Tensor\n",
    "    beta = torch.tensor(0.1, device=device)  # Umwandlung in Tensor\n",
    "\n",
    "    # Iteriere über die Timesteps\n",
    "    for t in reversed(range(timesteps)):\n",
    "        t_batch = torch.full((n,), t, device=device, dtype=torch.float)  # Zeitinformationen für diesen Schritt\n",
    "        t_emb = model.time_mlp(t_batch)  # Berechne die Zeitembeddings\n",
    "\n",
    "        # Zeiteinbettung für das Modell\n",
    "        t1 = model.time_linear1(t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        t2 = model.time_linear2(t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # Vorhersage des Rauschens\n",
    "        pred_noise = model(x, t_batch)  # Vorhersage des Rauschens (x_out enthält das bereinigte Bild)\n",
    "        \n",
    "        # Berechne das Bild für den nächsten Schritt\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)  # Füge Rauschen hinzu, außer im letzten Schritt\n",
    "        else:\n",
    "            noise = 0\n",
    "\n",
    "        # Verfeinerung des Bildes unter Verwendung des vorhersagten Rauschens\n",
    "        x = (1 / torch.sqrt(alpha)) * (x - ((1 - alpha) / torch.sqrt(1 - alpha_hat)) * pred_noise) + torch.sqrt(beta) * noise\n",
    "\n",
    "    return x\n",
    "\n",
    "# Initialisiere Modell und verschiebe es auf das richtige Gerät\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet().to(device)\n",
    "\n",
    "# Generiere ein Sample\n",
    "samples = sample(model, img_size=32, n=1, timesteps=1000)\n",
    "\n",
    "# Visualisiere das Ergebnis\n",
    "import matplotlib.pyplot as plt\n",
    "samples = (samples.clamp(-1, 1) + 1) / 2  # Skaliere zurück auf [0, 1]\n",
    "plt.imshow(samples[0].permute(1, 2, 0).cpu().detach().numpy())  # Zeige das erste Sample\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module.eval() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     19\u001b[39m         x = (\u001b[32m1\u001b[39m / torch.sqrt(alpha)) * (x - ((\u001b[32m1\u001b[39m - alpha) / torch.sqrt(\u001b[32m1\u001b[39m - alpha_hat)) * pred_noise) + torch.sqrt(beta) * noise\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m samples = \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUNet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m samples = (samples.clamp(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m) + \u001b[32m1\u001b[39m) / \u001b[32m2\u001b[39m  \u001b[38;5;66;03m# zurück auf [0,1]\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Visualisiere ein Beispiel\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36msample\u001b[39m\u001b[34m(model, img_size, n)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample\u001b[39m( model, img_size=\u001b[32m32\u001b[39m, n=\u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     x = torch.randn((n, \u001b[32m3\u001b[39m, img_size, img_size)).to(device)  \u001b[38;5;66;03m# 3 Kanäle für RGB\u001b[39;00m\n\u001b[32m      5\u001b[39m     timesteps = \u001b[32m1000\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: Module.eval() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
