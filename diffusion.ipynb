{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Karten-Generierung mit Diffusionsmodell (INRIA Aerial Dataset)\n",
    "\n",
    "Dieses Notebook trainiert ein einfaches Diffusionsmodell auf Stadt-Luftbildern.\n",
    "Optional können Gebäude-Masken als Bedingung verwendet werden (Skizzen oder Kontrolle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\Code\\DiffusionModel\\.venv\\Lib\\site-packages\\PIL\\ImageFile.py:644\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(im, fp, tile, bufsize)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     fh = \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfileno\u001b[49m()\n\u001b[32m    645\u001b[39m     fp.flush()\n",
      "\u001b[31mAttributeError\u001b[39m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ((x, y), patch_img), ((_, _), patch_mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(img_patches, mask_patches):\n\u001b[32m     47\u001b[39m         filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbasename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m         \u001b[43mpatch_img\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_img_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m         patch_mask.save(os.path.join(output_mask_dir, filename))\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAlle Patches wurden erfolgreich erstellt und gespeichert.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\Code\\DiffusionModel\\.venv\\Lib\\site-packages\\PIL\\Image.py:2588\u001b[39m, in \u001b[36mImage.save\u001b[39m\u001b[34m(self, fp, format, **params)\u001b[39m\n\u001b[32m   2585\u001b[39m     fp = cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n\u001b[32m   2587\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2588\u001b[39m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2589\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   2590\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\Code\\DiffusionModel\\.venv\\Lib\\site-packages\\PIL\\PngImagePlugin.py:1495\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(im, fp, filename, chunk, save_all)\u001b[39m\n\u001b[32m   1491\u001b[39m     single_im = _write_multiple_frames(\n\u001b[32m   1492\u001b[39m         im, fp, chunk, mode, rawmode, default_image, append_images\n\u001b[32m   1493\u001b[39m     )\n\u001b[32m   1494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m single_im:\n\u001b[32m-> \u001b[39m\u001b[32m1495\u001b[39m     \u001b[43mImageFile\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1496\u001b[39m \u001b[43m        \u001b[49m\u001b[43msingle_im\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIO\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1498\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mImageFile\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Tile\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_im\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1501\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[32m   1502\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info.chunks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\Code\\DiffusionModel\\.venv\\Lib\\site-packages\\PIL\\ImageFile.py:648\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(im, fp, tile, bufsize)\u001b[39m\n\u001b[32m    646\u001b[39m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io.UnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m     \u001b[43m_encode_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[33m\"\u001b[39m\u001b[33mflush\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    650\u001b[39m     fp.flush()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\Code\\DiffusionModel\\.venv\\Lib\\site-packages\\PIL\\ImageFile.py:674\u001b[39m, in \u001b[36m_encode_tile\u001b[39m\u001b[34m(im, fp, tile, bufsize, fh, exc)\u001b[39m\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[32m    672\u001b[39m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m         errcode, data = \u001b[43mencoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m:]\n\u001b[32m    675\u001b[39m         fp.write(data)\n\u001b[32m    676\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "input_dir = \"data/aerialimagelabeling/train/\" \n",
    "\n",
    "\n",
    "output_img_dir = \"data/aerialimagelabeling/transformed/rgb_patches_128_overlap64\"\n",
    "output_mask_dir = \"data/aerialimagelabeling/transformed/mask_patches_128_overlap64\"\n",
    "\n",
    "\n",
    "os.makedirs(output_img_dir, exist_ok=True)\n",
    "os.makedirs(output_mask_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_patches(img, tile_size=128, overlap=64):\n",
    "    step = tile_size-overlap\n",
    "    patches = []\n",
    "\n",
    "    for y in range(0, img.height - tile_size + 1, step):\n",
    "        for x in range(0, img.width - tile_size + 1, step):\n",
    "            patch = img.crop((x, y, x + tile_size, y + tile_size))\n",
    "            patches.append(((x,y), patch))\n",
    "    return patches\n",
    "\n",
    "#Alle *_sat.jpg Bilder finden und dazugehörige Masken verarbeiten \n",
    "image_paths = sorted(glob(os.path.join(input_dir, \"*_sat.jpg\")))\n",
    "\n",
    "for img_path in image_paths:\n",
    "    basename = os.path.basename(img_path).replace(\"_sat.jpg\", \"\")\n",
    "    mask_path = os.path.join(input_dir, f\"{basename}_mask.png\")\n",
    "    \n",
    "    if not os.path.exists(mask_path):\n",
    "        print(f\"⚠️ Maske fehlt für {basename}, überspringe...\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "    img_patches = extract_patches(image)\n",
    "    mask_patches = extract_patches(mask)\n",
    "\n",
    "    \n",
    "    # Speichern\n",
    "    for ((x, y), patch_img), ((_, _), patch_mask) in zip(img_patches, mask_patches):\n",
    "        filename = f\"{basename}_{x}_{y}.png\"\n",
    "        patch_img.save(os.path.join(output_img_dir, filename))\n",
    "        patch_mask.save(os.path.join(output_mask_dir, filename))\n",
    "\n",
    "print(\"Alle Patches wurden erfolgreich erstellt und gespeichert.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testbild erfolgreich in Patches zerlegt und gespeichert.\n"
     ]
    }
   ],
   "source": [
    "#Einzelnes Bild verarbeiten\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# === 1. Test-Dateien definieren ===\n",
    "\n",
    "input_dir = \"data/aerialimagelabeling/train/\" \n",
    "\n",
    "\n",
    "output_img_dir = \"data/aerialimagelabeling/transformed/rgb_patches_128_overlap64\"\n",
    "output_mask_dir = \"data/aerialimagelabeling/transformed/mask_patches_128_overlap64\"\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(output_img_dir, exist_ok=True)\n",
    "os.makedirs(output_mask_dir, exist_ok=True)\n",
    "\n",
    "# === 2. Datei-ID festlegen ===\n",
    "basename = \"104\"\n",
    "img_path = os.path.join(input_dir, f\"{basename}_sat.jpg\")\n",
    "mask_path = os.path.join(input_dir, f\"{basename}_mask.png\")\n",
    "\n",
    "# === 3. Bild und Maske laden ===\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "# === 4. Funktion zum sicheren Zuschneiden ===\n",
    "def extract_patches(img, tile_size=128, overlap=64):\n",
    "    step = tile_size - overlap\n",
    "    patches = []\n",
    "    w, h = img.size\n",
    "    for y in range(0, h, step):\n",
    "        for x in range(0, w, step):\n",
    "            if x + tile_size <= w and y + tile_size <= h:\n",
    "                patch = img.crop((x, y, x + tile_size, y + tile_size))\n",
    "                patches.append(((x, y), patch))\n",
    "    return patches\n",
    "\n",
    "# === 5. Patches erzeugen ===\n",
    "img_patches = extract_patches(image)\n",
    "mask_patches = extract_patches(mask)\n",
    "\n",
    "# === 6. Ergebnisse speichern ===\n",
    "for i, (((x, y), patch_img), ((_, _), patch_mask)) in enumerate(zip(img_patches, mask_patches)):\n",
    "    part_name = f\"{basename}_Part_{i+1}\"\n",
    "    \n",
    "    # Zielordner für dieses Patch\n",
    "    img_patch_dir = os.path.join(output_img_dir, basename, part_name)\n",
    "    mask_patch_dir = os.path.join(output_mask_dir, basename, part_name)\n",
    "    os.makedirs(img_patch_dir, exist_ok=True)\n",
    "    os.makedirs(mask_patch_dir, exist_ok=True)\n",
    "\n",
    "    # Speichern mit Standardnamen (z. B. image.png / mask.png)\n",
    "    patch_img.save(os.path.join(img_patch_dir, \"image.png\"))\n",
    "    patch_mask.save(os.path.join(mask_patch_dir, \"mask.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version2807\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# === 1. Verzeichnisse festlegen ===\n",
    "\n",
    "input_dir = \"data/aerialimagelabeling/train/\" \n",
    "output_img_dir = \"data/aerialimagelabeling/transformed/rgb_patches_128\"\n",
    "output_mask_dir = \"data/aerialimagelabeling/transformed/mask_patches_128\"\n",
    "\n",
    "os.makedirs(output_img_dir, exist_ok=True)\n",
    "os.makedirs(output_mask_dir, exist_ok=True)\n",
    "\n",
    "# === 2. Datei-ID festlegen ===\n",
    "basename = \"104\"\n",
    "img_path = os.path.join(input_dir, f\"{basename}_sat.jpg\")\n",
    "mask_path = os.path.join(input_dir, f\"{basename}_mask.png\")\n",
    "\n",
    "# === 3. Bild und Maske laden ===\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "# === 4. Funktion für Patches ohne Overlap ===\n",
    "def extract_patches_no_overlap(img, tile_size=128):\n",
    "    patches = []\n",
    "    w, h = img.size\n",
    "\n",
    "    # Maximaler Bereich, der durch tile_size ohne Rest teilbar ist\n",
    "    max_x = w - (w % tile_size)\n",
    "    max_y = h - (h % tile_size)\n",
    "\n",
    "    for y in range(0, max_y, tile_size):\n",
    "        for x in range(0, max_x, tile_size):\n",
    "            patch = img.crop((x, y, x + tile_size, y + tile_size))\n",
    "            patches.append(((x, y), patch))\n",
    "    return patches\n",
    "\n",
    "# === 5. Patches erzeugen ===\n",
    "img_patches = extract_patches_no_overlap(image)\n",
    "mask_patches = extract_patches_no_overlap(mask)\n",
    "\n",
    "# === 6. Ergebnisse speichern ===\n",
    "for i, (((x, y), patch_img), ((_, _), patch_mask)) in enumerate(zip(img_patches, mask_patches)):\n",
    "    part_name = f\"{basename}_Part_{i+1}\"\n",
    "    \n",
    "    # Zielordner für dieses Patch\n",
    "    img_patch_dir = os.path.join(output_img_dir, basename, part_name)\n",
    "    mask_patch_dir = os.path.join(output_mask_dir, basename, part_name)\n",
    "    os.makedirs(img_patch_dir, exist_ok=True)\n",
    "    os.makedirs(mask_patch_dir, exist_ok=True)\n",
    "\n",
    "    # Speichern mit Standardnamen\n",
    "    patch_img.save(os.path.join(img_patch_dir, \"image.png\"))\n",
    "    patch_mask.save(os.path.join(mask_patch_dir, \"mask.png\"))\n",
    "\n",
    "print(\"Fertig: Alle 128x128-Patches ohne Überlappung gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: (t- 0.5) * 2),\n",
    "])\n",
    "\n",
    "train_dataset = MNIST(root=\"data\", train=True, download=True, transform=transform )\n",
    "dataloader = DataLoader(train_dataset, batch_size =32, shuffle=True)\n",
    "\n",
    "\n",
    "target_class = 3\n",
    "\n",
    "# 🔧 Filtere alle Indizes mit y == 3\n",
    "filtered_indices = [i for i in range(len(train_dataset)) if train_dataset.targets[i] == target_class]\n",
    "\n",
    "# Erstelle Subset\n",
    "filtered_dataset = Subset(train_dataset, filtered_indices)\n",
    "\n",
    "# DataLoader\n",
    "filtered_dataloader = DataLoader(filtered_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class MaskToRGBDataset(Dataset):\n",
    "\n",
    "    def __init__(self, mask_root, rgb_root, transform=None):\n",
    "\n",
    "        self.transform = transform if transform else T.Compose([\n",
    "            T.Resize((128,128)),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        self.paths = []\n",
    "\n",
    "        for folder in os.listdir(mask_root):\n",
    "            mask_dir = os.path.join(mask_root, folder)\n",
    "            rgb_dir = os.path.join(rgb_root, folder)\n",
    "            if not os.path.isdir(mask_dir):\n",
    "                continue\n",
    "            for fname in os.listdir(mask_dir):\n",
    "                mask_path = os.path.join(mask_dir, fname)\n",
    "                rgb_path = os.path.join(rgb_dir, fname)\n",
    "                if os.path.exists(rgb_path):\n",
    "                    self.paths.append((mask_path, rgb_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mask_path, rgb_path = self.paths[idx]\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        rgb = Image.open(rgb_path).convert(\"RGB\")\n",
    "\n",
    "        mask = self.transform(mask) * 2 - 1  # [-1, 1]\n",
    "        rgb = self.transform(rgb) * 2 - 1\n",
    "\n",
    "        #return mask, rgb\n",
    "        return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "T = 1000\n",
    "betas = torch.linspace(1e-4, 0.02, T)   #Linear Noise\n",
    "alphas = 1 - betas\n",
    "alpha_hat = torch.cumprod(alphas, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class DDPM:\n",
    "    def __init__(self, model, n_steps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.model = model\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "        # Lineare Noise-Rate\n",
    "        self.betas = torch.linspace(beta_start, beta_end, n_steps)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alpha_hat = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise):\n",
    "        \"\"\"\n",
    "        Simuliert ein verrauschtes Bild zu Zeit t\n",
    "        \"\"\"\n",
    "        sqrt_alpha_hat = self.alpha_hat[t].sqrt().view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus = (1 - self.alpha_hat[t]).sqrt().view(-1, 1, 1, 1)\n",
    "        return sqrt_alpha_hat * x_start + sqrt_one_minus * noise\n",
    "\n",
    "    #def p_losses(self, x_start, mask_cond, t):\n",
    "    def p_losses(self, x_start, t): \n",
    "        \"\"\"\n",
    "        Trainingsloss: vorhergesagter vs. tatsächlicher Rauschterm\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(x_start)\n",
    "        x_noisy = self.q_sample(x_start, t, noise)\n",
    "\n",
    "        # Input ist Maske + verrauschtes Bild\n",
    "        #model_input = torch.cat([mask_cond, x_noisy], dim=1)\n",
    "        #predicted_noise = self.model(model_input, t)\n",
    "\n",
    "        predicted_noise = self.model(x_noisy, t)\n",
    "        return torch.nn.functional.mse_loss(predicted_noise, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion_sample(x_0, t, noise):\n",
    "    sqrt_alpha_hat = torch.sqrt(alpha_hat[t])[:,None, None, None]\n",
    "    sqrt_one_minus_alpha_hat = torch.sqrt(1- alpha_hat[t])[:, None, None, None]\n",
    "    \n",
    "    return sqrt_alpha_hat * x_0 + sqrt_one_minus_alpha_hat * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28 * 28)\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Zeitstempel als Feature\n",
    "        t = t.float().view(-1, 1) / 1000\n",
    "        time_embedding = self.time_mlp(t).view(-1, 1, 28, 28)\n",
    "        x = torch.cat([x, time_embedding], dim=1)\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#U Net\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256)\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = conv_block(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)  # 28x28 → 14x14\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)  # 14x14 → 7x7\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = conv_block(128, 256)\n",
    "\n",
    "        # Decoder\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)  # 7x7 → 14x14\n",
    "        self.dec2 = conv_block(256, 128)  # 128 von upsample + 128 von enc2\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)   # 14x14 → 28x28\n",
    "        self.dec1 = conv_block(128, 64)  # 64 von upsample + 64 von enc1\n",
    "\n",
    "        # Output-Schicht\n",
    "        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \n",
    "        t = t.float().view(-1, 1) / 1000\n",
    "        time_embed = self.time_mlp(t).view(-1, 256, 1, 1)\n",
    "\n",
    "\n",
    "        # Encoder\n",
    "        x1 = self.enc1(x)                 # [B, 64, 28, 28]\n",
    "        x2 = self.enc2(self.pool1(x1))    # [B, 128, 14, 14]\n",
    "        x3 = self.bottleneck(self.pool2(x2))  + time_embed  # [B, 256, 7, 7]\n",
    "\n",
    "        # Decoder\n",
    "        x = self.up2(x3)                  # [B, 128, 14, 14]\n",
    "        x = torch.cat([x, x2], dim=1)     # [B, 256, 14, 14]\n",
    "        x = self.dec2(x)                  # [B, 128, 14, 14]\n",
    "\n",
    "        x = self.up1(x)                   # [B, 64, 28, 28]\n",
    "        x = torch.cat([x, x1], dim=1)     # [B, 128, 28, 28]\n",
    "        x = self.dec1(x)                  # [B, 64, 28, 28]\n",
    "\n",
    "        return self.final(x)              # [B, 1, 28, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNet RGB\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "class UNetRGB(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=3):  \n",
    "        super().__init__()\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256)\n",
    "        )\n",
    "\n",
    "        self.enc1 = conv_block(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.bottleneck = conv_block(128, 256)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = conv_block(256, 128)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = conv_block(128, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = t.float().view(-1, 1) / 1000  \n",
    "        time_embed = self.time_mlp(t).view(-1, 256, 1, 1)\n",
    "\n",
    "        x1 = self.enc1(x)\n",
    "        x2 = self.enc2(self.pool1(x1))\n",
    "        x3 = self.bottleneck(self.pool2(x2)) + time_embed\n",
    "        x = self.up2(x3)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.dec2(x)\n",
    "        x = self.up1(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.dec1(x)\n",
    "        return self.final(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    all_losses = []\n",
    "    for epoch in range(5):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        progress_bar = tqdm(dataloader)\n",
    "        for x, _ in progress_bar:\n",
    "            x = x.to(device)\n",
    "            t = torch.randint(0, T, (x.size(0),), device=device).long()\n",
    "            noise = torch.randn_like(x)\n",
    "            x_noisy = forward_diffusion_sample(x, t, noise)\n",
    "            noise_pred = model(x_noisy, t)\n",
    "            loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            all_losses.append(loss.item())  # speichere Loss\n",
    "            progress_bar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "            \n",
    "    return all_losses  # gibt am Ende alle Loss-Werte zurück"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:01<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 0.9831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:39<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Loss: 0.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:39<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Loss: 0.7806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:38<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Loss: 0.6729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:38<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Loss: 0.6754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:38<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Loss: 0.2352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:38<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Loss: 0.1465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:38<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Loss: 0.1360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:38<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Loss: 0.0769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:37<00:00,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Loss: 0.1779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Train RGB\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "\n",
    "mask_root = \"data/aerialimagelabeling/transformed/rgb_patches_128_overlap64\"\n",
    "rgb_root = \"data/aerialimagelabeling/transformed/mask_patches_128_overlap64\"\n",
    "batch_size = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Daten laden\n",
    "dataset = MaskToRGBDataset(mask_root, rgb_root)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#odell & Diffusion\n",
    "#model = UNetRGB(in_channels=4, out_channels=3).to(device)\n",
    "\n",
    "model = UNetRGB(in_channels=3, out_channels=3).to(device)\n",
    "\n",
    "ddpm = DDPM(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    # for mask, rgb in tqdm.tqdm(dataloader):\n",
    "    #     mask, rgb = mask.to(device), rgb.to(device)\n",
    "\n",
    "    #     t = torch.randint(0, ddpm.n_steps, (rgb.size(0),), device=device).long()\n",
    "    #     loss = ddpm.p_losses(x_start=rgb, mask_cond=mask, t=t)\n",
    "    for rgb in tqdm.tqdm(dataloader):\n",
    "        rgb = rgb.to(device)\n",
    "        t = torch.randint(0, ddpm.n_steps, (rgb.size(0),), device=device).long()\n",
    "        loss = ddpm.p_losses(x_start=rgb, t=t) \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, shape, device):\n",
    "    x = torch.randn(shape).to(device)  # Start mit reinem Rauschen\n",
    "    for t in reversed(range(T)):\n",
    "        t_batch = torch.full((shape[0],), t, device=device, dtype=torch.long)\n",
    "        noise_pred = model(x, t_batch)\n",
    "        beta_t = betas[t]\n",
    "        alpha_t = alphas[t]\n",
    "        alpha_hat_t = alpha_hat[t]\n",
    "        x = (1 / torch.sqrt(alpha_t)) * (x - ((1 - alpha_t) / torch.sqrt(1 - alpha_hat_t)) * noise_pred)\n",
    "        if t > 0:\n",
    "            z = torch.randn_like(x)\n",
    "            x += torch.sqrt(beta_t) * z\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_from_model(ddpm, mask, device, n_steps=1000):\n",
    "    model = ddpm.model\n",
    "    model.eval()\n",
    "\n",
    "    # Bildgröße und Batch ableiten\n",
    "    B, _, H, W = mask.shape\n",
    "\n",
    "    # 1. Initialisiere RGB-Bild mit reinem Rauschen\n",
    "    x = torch.randn(B, 3, H, W).to(device)\n",
    "\n",
    "    for t_step in reversed(range(n_steps)):\n",
    "        t = torch.full((B,), t_step, device=device, dtype=torch.long)\n",
    "\n",
    "        # Verbinde Maske + momentanen Bildzustand\n",
    "        model_input = torch.cat([mask, x], dim=1)\n",
    "        predicted_noise = model(model_input, t)\n",
    "\n",
    "        alpha = ddpm.alphas[t].view(-1, 1, 1, 1)\n",
    "        alpha_hat = ddpm.alpha_hat[t].view(-1, 1, 1, 1)\n",
    "        beta = ddpm.betas[t].view(-1, 1, 1, 1)\n",
    "\n",
    "        # Rekonstruiere vorherigen Zustand\n",
    "        x = (1 / alpha.sqrt()) * (x - (1 - alpha) / (1 - alpha_hat).sqrt() * predicted_noise)\n",
    "\n",
    "        # Falls nicht der letzte Schritt → füge leichtes neues Rauschen hinzu\n",
    "        if t_step > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            x += beta.sqrt() * noise\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling for Model that learned only with noise \n",
    "\n",
    "torch.no_grad()\n",
    "def sample_from_model(ddpm, device, n_steps=1000, shape=(1, 3, 128, 128)):\n",
    "    model = ddpm.model\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.randn(shape).to(device)\n",
    "\n",
    "    for t_step in reversed(range(n_steps)):\n",
    "        t = torch.full((shape[0],), t_step, device=device, dtype=torch.long)\n",
    "        predicted_noise = model(x, t)\n",
    "\n",
    "        alpha = ddpm.alphas[t].view(-1, 1, 1, 1)\n",
    "        alpha_hat = ddpm.alpha_hat[t].view(-1, 1, 1, 1)\n",
    "        beta = ddpm.betas[t].view(-1, 1, 1, 1)\n",
    "\n",
    "        x = (1 / alpha.sqrt()) * (x - (1 - alpha) / (1 - alpha_hat).sqrt() * predicted_noise)\n",
    "\n",
    "        if t_step > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            x += beta.sqrt() * noise\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch.device' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m mask_tensor = transform(mask_image).unsqueeze(\u001b[32m0\u001b[39m).to(device) * \u001b[32m2\u001b[39m - \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# [1, 1, 128, 128]\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Samplen\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m generated_rgb = \u001b[43msample_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mddpm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# → [1, 3, 128, 128]\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# zurückskalieren und anzeigen\u001b[39;00m\n\u001b[32m     13\u001b[39m to_img = \u001b[38;5;28;01mlambda\u001b[39;00m x: (x.clamp(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m) + \u001b[32m1\u001b[39m) / \u001b[32m2\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36msample_from_model\u001b[39m\u001b[34m(ddpm, device, n_steps, shape)\u001b[39m\n\u001b[32m      6\u001b[39m model.eval()\n\u001b[32m      8\u001b[39m x = torch.randn(shape).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m     11\u001b[39m     t = torch.full((shape[\u001b[32m0\u001b[39m],), t_step, device=device, dtype=torch.long)\n\u001b[32m     12\u001b[39m     predicted_noise = model(x, t)\n",
      "\u001b[31mTypeError\u001b[39m: 'torch.device' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# Beispielmaske laden\n",
    "mask_image = Image.open(\"data/aerialimagelabeling/transformed/mask_patches_128_overlap64/104/104_Part_81.png\").convert(\"L\")\n",
    "transform = T.Compose([\n",
    "    T.Resize((128, 128)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "mask_tensor = transform(mask_image).unsqueeze(0).to(device) * 2 - 1  # [1, 1, 128, 128]\n",
    "\n",
    "# Samplen\n",
    "generated_rgb = sample_from_model(ddpm, mask_tensor, device)  # → [1, 3, 128, 128]\n",
    "\n",
    "# zurückskalieren und anzeigen\n",
    "to_img = lambda x: (x.clamp(-1, 1) + 1) / 2\n",
    "img = to_img(generated_rgb[0]).cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title(\"Generiertes RGB-Bild\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Trainieren\n",
    "losses = train(model, dataloader, optimizer, device)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Funktion zum Zurückskalieren [-1, 1] → [0, 1]\n",
    "to_img = lambda x: (x.clamp(-1, 1) + 1) / 2\n",
    "\n",
    "\n",
    "generated_rgb = sample_from_model(ddpm, device, shape=(3, 3, 128, 128))  # 🔄 shape = 3 Bilder\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for i in range(1):\n",
    "    img = to_img(generated_rgb[i]).cpu().permute(1, 2, 0).numpy()\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(f\"Sample {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "samples = sample(model, shape=(16, 1, 28, 28), device=device)\n",
    "\n",
    "for i in range(16):\n",
    "    plt.imshow((samples[i][0].cpu().numpy() + 1) / 2, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
