{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Karten-Generierung mit Diffusionsmodell (INRIA Aerial Dataset)\n",
    "\n",
    "Dieses Notebook trainiert ein einfaches Diffusionsmodell auf Stadt-Luftbildern.\n",
    "Optional k√∂nnen Geb√§ude-Masken als Bedingung verwendet werden (Skizzen oder Kontrolle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "input_dir = \"data/aerialimagelabeling/train/\" \n",
    "\n",
    "\n",
    "output_img_dir = \"data/aerialimagelabeling/transformed/rgb_patches_128_overlap64\"\n",
    "output_mask_dir = \"data/aerialimagelabeling/transformed/mask_patches_128_overlap64\"\n",
    "\n",
    "\n",
    "os.makedirs(output_img_dir, exist_ok=True)\n",
    "os.makedirs(output_mask_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_patches(img, tile_size=128, overlap=64):\n",
    "    step = tile_size-overlap\n",
    "    patches = []\n",
    "\n",
    "    for y in range(0, img.height - tile_size + 1, step):\n",
    "        for x in range(0, img.width - tile_size + 1, step):\n",
    "            patch = img.crop((x, y, x + tile_size, y + tile_size))\n",
    "            patches.append(((x,y), patch))\n",
    "    return patches\n",
    "\n",
    "#Alle *_sat.jpg Bilder finden und dazugeh√∂rige Masken verarbeiten \n",
    "image_paths = sorted(glob(os.path.join(input_dir, \"*_sat.jpg\")))\n",
    "\n",
    "for img_path in image_paths:\n",
    "    basename = os.path.basename(img_path).replace(\"_sat.jpg\", \"\")\n",
    "    mask_path = os.path.join(input_dir, f\"{basename}_mask.png\")\n",
    "    \n",
    "    if not os.path.exists(mask_path):\n",
    "        print(f\"‚ö†Ô∏è Maske fehlt f√ºr {basename}, √ºberspringe...\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "    img_patches = extract_patches(image)\n",
    "    mask_patches = extract_patches(mask)\n",
    "\n",
    "    \n",
    "    # Speichern\n",
    "    for ((x, y), patch_img), ((_, _), patch_mask) in zip(img_patches, mask_patches):\n",
    "        filename = f\"{basename}_{x}_{y}.png\"\n",
    "        patch_img.save(os.path.join(output_img_dir, filename))\n",
    "        patch_mask.save(os.path.join(output_mask_dir, filename))\n",
    "\n",
    "print(\"Alle Patches wurden erfolgreich erstellt und gespeichert.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testbild erfolgreich in Patches zerlegt und gespeichert.\n"
     ]
    }
   ],
   "source": [
    "#Einzelnes Bild verarbeiten\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# === 1. Test-Dateien definieren ===\n",
    "\n",
    "input_dir = \"data/aerialimagelabeling/train/\" \n",
    "\n",
    "\n",
    "output_img_dir = \"data/aerialimagelabeling/transformed/rgb_patches_128_overlap64\"\n",
    "output_mask_dir = \"data/aerialimagelabeling/transformed/mask_patches_128_overlap64\"\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(output_img_dir, exist_ok=True)\n",
    "os.makedirs(output_mask_dir, exist_ok=True)\n",
    "\n",
    "# === 2. Datei-ID festlegen ===\n",
    "basename = \"104\"\n",
    "img_path = os.path.join(input_dir, f\"{basename}_sat.jpg\")\n",
    "mask_path = os.path.join(input_dir, f\"{basename}_mask.png\")\n",
    "\n",
    "# === 3. Bild und Maske laden ===\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "# === 4. Funktion zum sicheren Zuschneiden ===\n",
    "def extract_patches(img, tile_size=128, overlap=64):\n",
    "    step = tile_size - overlap\n",
    "    patches = []\n",
    "    w, h = img.size\n",
    "    for y in range(0, h, step):\n",
    "        for x in range(0, w, step):\n",
    "            if x + tile_size <= w and y + tile_size <= h:\n",
    "                patch = img.crop((x, y, x + tile_size, y + tile_size))\n",
    "                patches.append(((x, y), patch))\n",
    "    return patches\n",
    "\n",
    "# === 5. Patches erzeugen ===\n",
    "img_patches = extract_patches(image)\n",
    "mask_patches = extract_patches(mask)\n",
    "\n",
    "# === 6. Ergebnisse speichern ===\n",
    "for i, (((x, y), patch_img), ((_, _), patch_mask)) in enumerate(zip(img_patches, mask_patches)):\n",
    "    filename = f\"{basename}_Part_{i+1}.png\"\n",
    "\n",
    "    # Ordner f√ºr dieses Bild anlegen (einmalig)\n",
    "    img_output_subdir = os.path.join(output_img_dir, basename)\n",
    "    mask_output_subdir = os.path.join(output_mask_dir, basename)\n",
    "    os.makedirs(img_output_subdir, exist_ok=True)\n",
    "    os.makedirs(mask_output_subdir, exist_ok=True)\n",
    "\n",
    "    # Dateien speichern in jeweiligem Unterordner\n",
    "    patch_img.save(os.path.join(img_output_subdir, filename))\n",
    "    patch_mask.save(os.path.join(mask_output_subdir, filename))\n",
    "\n",
    "print(\"Testbild erfolgreich in Patches zerlegt und gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: (t- 0.5) * 2),\n",
    "])\n",
    "\n",
    "train_dataset = MNIST(root=\"data\", train=True, download=True, transform=transform )\n",
    "dataloader = DataLoader(train_dataset, batch_size =32, shuffle=True)\n",
    "\n",
    "\n",
    "target_class = 3\n",
    "\n",
    "# üîß Filtere alle Indizes mit y == 3\n",
    "filtered_indices = [i for i in range(len(train_dataset)) if train_dataset.targets[i] == target_class]\n",
    "\n",
    "# Erstelle Subset\n",
    "filtered_dataset = Subset(train_dataset, filtered_indices)\n",
    "\n",
    "# DataLoader\n",
    "filtered_dataloader = DataLoader(filtered_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class MaskToRGBDataset(Dataset):\n",
    "\n",
    "    def __init__(self, mask_root, rgb_root, transform=None):\n",
    "\n",
    "        self.transform = transform if transform else T.Compose([\n",
    "            T.Resize((128,128)),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        self.paths = []\n",
    "\n",
    "        for folder in os.listdir(mask_root):\n",
    "            mask_dir = os.path.join(mask_root, folder)\n",
    "            rgb_dir = os.path.join(rgb_root, folder)\n",
    "            if not os.path.isdir(mask_dir):\n",
    "                continue\n",
    "            for fname in os.listdir(mask_dir):\n",
    "                mask_path = os.path.join(mask_dir, fname)\n",
    "                rgb_path = os.path.join(rgb_dir, fname)\n",
    "                if os.path.exists(rgb_path):\n",
    "                    self.paths.append((mask_path, rgb_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mask_path, rgb_path = self.paths[idx]\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        rgb = Image.open(rgb_path).convert(\"RGB\")\n",
    "\n",
    "        mask = self.transform(mask) * 2 - 1  # [-1, 1]\n",
    "        rgb = self.transform(rgb) * 2 - 1\n",
    "\n",
    "        #return mask, rgb\n",
    "        return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "T = 1000\n",
    "betas = torch.linspace(1e-4, 0.02, T)   #Linear Noise\n",
    "alphas = 1 - betas\n",
    "alpha_hat = torch.cumprod(alphas, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class DDPM:\n",
    "    def __init__(self, model, n_steps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.model = model\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "        # Lineare Noise-Rate\n",
    "        self.betas = torch.linspace(beta_start, beta_end, n_steps)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alpha_hat = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise):\n",
    "        \"\"\"\n",
    "        Simuliert ein verrauschtes Bild zu Zeit t\n",
    "        \"\"\"\n",
    "        sqrt_alpha_hat = self.alpha_hat[t].sqrt().view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus = (1 - self.alpha_hat[t]).sqrt().view(-1, 1, 1, 1)\n",
    "        return sqrt_alpha_hat * x_start + sqrt_one_minus * noise\n",
    "\n",
    "    def p_losses(self, x_start, mask_cond, t):\n",
    "        \"\"\"\n",
    "        Trainingsloss: vorhergesagter vs. tats√§chlicher Rauschterm\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(x_start)\n",
    "        x_noisy = self.q_sample(x_start, t, noise)\n",
    "\n",
    "        # Input ist Maske + verrauschtes Bild\n",
    "        #model_input = torch.cat([mask_cond, x_noisy], dim=1)\n",
    "        #predicted_noise = self.model(model_input, t)\n",
    "\n",
    "        predicted_noise = self.model(x_noisy, t)\n",
    "        return torch.nn.functional.mse_loss(predicted_noise, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion_sample(x_0, t, noise):\n",
    "    sqrt_alpha_hat = torch.sqrt(alpha_hat[t])[:,None, None, None]\n",
    "    sqrt_one_minus_alpha_hat = torch.sqrt(1- alpha_hat[t])[:, None, None, None]\n",
    "    \n",
    "    return sqrt_alpha_hat * x_0 + sqrt_one_minus_alpha_hat * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28 * 28)\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Zeitstempel als Feature\n",
    "        t = t.float().view(-1, 1) / 1000\n",
    "        time_embedding = self.time_mlp(t).view(-1, 1, 28, 28)\n",
    "        x = torch.cat([x, time_embedding], dim=1)\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#U Net\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256)\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = conv_block(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)  # 28x28 ‚Üí 14x14\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)  # 14x14 ‚Üí 7x7\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = conv_block(128, 256)\n",
    "\n",
    "        # Decoder\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)  # 7x7 ‚Üí 14x14\n",
    "        self.dec2 = conv_block(256, 128)  # 128 von upsample + 128 von enc2\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)   # 14x14 ‚Üí 28x28\n",
    "        self.dec1 = conv_block(128, 64)  # 64 von upsample + 64 von enc1\n",
    "\n",
    "        # Output-Schicht\n",
    "        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \n",
    "        t = t.float().view(-1, 1) / 1000\n",
    "        time_embed = self.time_mlp(t).view(-1, 256, 1, 1)\n",
    "\n",
    "\n",
    "        # Encoder\n",
    "        x1 = self.enc1(x)                 # [B, 64, 28, 28]\n",
    "        x2 = self.enc2(self.pool1(x1))    # [B, 128, 14, 14]\n",
    "        x3 = self.bottleneck(self.pool2(x2))  + time_embed  # [B, 256, 7, 7]\n",
    "\n",
    "        # Decoder\n",
    "        x = self.up2(x3)                  # [B, 128, 14, 14]\n",
    "        x = torch.cat([x, x2], dim=1)     # [B, 256, 14, 14]\n",
    "        x = self.dec2(x)                  # [B, 128, 14, 14]\n",
    "\n",
    "        x = self.up1(x)                   # [B, 64, 28, 28]\n",
    "        x = torch.cat([x, x1], dim=1)     # [B, 128, 28, 28]\n",
    "        x = self.dec1(x)                  # [B, 64, 28, 28]\n",
    "\n",
    "        return self.final(x)              # [B, 1, 28, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNet RGB\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "class UNetRGB(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=3):  \n",
    "        super().__init__()\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256)\n",
    "        )\n",
    "\n",
    "        self.enc1 = conv_block(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.bottleneck = conv_block(128, 256)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = conv_block(256, 128)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = conv_block(128, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = t.float().view(-1, 1) / 1000  \n",
    "        time_embed = self.time_mlp(t).view(-1, 256, 1, 1)\n",
    "\n",
    "        x1 = self.enc1(x)\n",
    "        x2 = self.enc2(self.pool1(x1))\n",
    "        x3 = self.bottleneck(self.pool2(x2)) + time_embed\n",
    "        x = self.up2(x3)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.dec2(x)\n",
    "        x = self.up1(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.dec1(x)\n",
    "        return self.final(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    all_losses = []\n",
    "    for epoch in range(5):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        progress_bar = tqdm(dataloader)\n",
    "        for x, _ in progress_bar:\n",
    "            x = x.to(device)\n",
    "            t = torch.randint(0, T, (x.size(0),), device=device).long()\n",
    "            noise = torch.randn_like(x)\n",
    "            x_noisy = forward_diffusion_sample(x, t, noise)\n",
    "            noise_pred = model(x_noisy, t)\n",
    "            loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            all_losses.append(loss.item())  # speichere Loss\n",
    "            progress_bar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "            \n",
    "    return all_losses  # gibt am Ende alle Loss-Werte zur√ºck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:37<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 0.9782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:36<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Loss: 0.9017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:36<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Loss: 0.7844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:36<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Loss: 0.5821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:36<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Loss: 0.4075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:36<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Loss: 0.2168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:37<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Loss: 0.1971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:37<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Loss: 0.0741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:36<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Loss: 0.0595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:36<00:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Loss: 0.0465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Train RGB\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "\n",
    "mask_root = \"data/aerialimagelabeling/transformed/rgb_patches_128_overlap64\"\n",
    "rgb_root = \"data/aerialimagelabeling/transformed/mask_patches_128_overlap64\"\n",
    "batch_size = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Daten laden\n",
    "dataset = MaskToRGBDataset(mask_root, rgb_root)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#odell & Diffusion\n",
    "#model = UNetRGB(in_channels=4, out_channels=3).to(device)\n",
    "\n",
    "model = UNetRGB(in_channels=3, out_channels=3).to(device)\n",
    "\n",
    "ddpm = DDPM(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    for mask, rgb in tqdm.tqdm(dataloader):\n",
    "        mask, rgb = mask.to(device), rgb.to(device)\n",
    "\n",
    "        t = torch.randint(0, ddpm.n_steps, (rgb.size(0),), device=device).long()\n",
    "        loss = ddpm.p_losses(x_start=rgb, mask_cond=mask, t=t)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, shape, device):\n",
    "    x = torch.randn(shape).to(device)  # Start mit reinem Rauschen\n",
    "    for t in reversed(range(T)):\n",
    "        t_batch = torch.full((shape[0],), t, device=device, dtype=torch.long)\n",
    "        noise_pred = model(x, t_batch)\n",
    "        beta_t = betas[t]\n",
    "        alpha_t = alphas[t]\n",
    "        alpha_hat_t = alpha_hat[t]\n",
    "        x = (1 / torch.sqrt(alpha_t)) * (x - ((1 - alpha_t) / torch.sqrt(1 - alpha_hat_t)) * noise_pred)\n",
    "        if t > 0:\n",
    "            z = torch.randn_like(x)\n",
    "            x += torch.sqrt(beta_t) * z\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_from_model(ddpm, mask, device, n_steps=1000):\n",
    "    model = ddpm.model\n",
    "    model.eval()\n",
    "\n",
    "    # Bildgr√∂√üe und Batch ableiten\n",
    "    B, _, H, W = mask.shape\n",
    "\n",
    "    # 1. Initialisiere RGB-Bild mit reinem Rauschen\n",
    "    x = torch.randn(B, 3, H, W).to(device)\n",
    "\n",
    "    for t_step in reversed(range(n_steps)):\n",
    "        t = torch.full((B,), t_step, device=device, dtype=torch.long)\n",
    "\n",
    "        # Verbinde Maske + momentanen Bildzustand\n",
    "        model_input = torch.cat([mask, x], dim=1)\n",
    "        predicted_noise = model(model_input, t)\n",
    "\n",
    "        alpha = ddpm.alphas[t].view(-1, 1, 1, 1)\n",
    "        alpha_hat = ddpm.alpha_hat[t].view(-1, 1, 1, 1)\n",
    "        beta = ddpm.betas[t].view(-1, 1, 1, 1)\n",
    "\n",
    "        # Rekonstruiere vorherigen Zustand\n",
    "        x = (1 / alpha.sqrt()) * (x - (1 - alpha) / (1 - alpha_hat).sqrt() * predicted_noise)\n",
    "\n",
    "        # Falls nicht der letzte Schritt ‚Üí f√ºge leichtes neues Rauschen hinzu\n",
    "        if t_step > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            x += beta.sqrt() * noise\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Beispielmaske laden\n",
    "mask_image = Image.open(\"data/aerialimagelabeling/transformed/mask_patches_128_overlap64/104/104_Part_81.png\").convert(\"L\")\n",
    "transform = T.Compose([\n",
    "    T.Resize((128, 128)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "mask_tensor = transform(mask_image).unsqueeze(0).to(device) * 2 - 1  # [1, 1, 128, 128]\n",
    "\n",
    "# Samplen\n",
    "generated_rgb = sample_from_model(ddpm, mask_tensor, device)  # ‚Üí [1, 3, 128, 128]\n",
    "\n",
    "# zur√ºckskalieren und anzeigen\n",
    "to_img = lambda x: (x.clamp(-1, 1) + 1) / 2\n",
    "img = to_img(generated_rgb[0]).cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title(\"Generiertes RGB-Bild\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Trainieren\n",
    "losses = train(model, dataloader, optimizer, device)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "samples = sample(model, shape=(16, 1, 28, 28), device=device)\n",
    "\n",
    "for i in range(16):\n",
    "    plt.imshow((samples[i][0].cpu().numpy() + 1) / 2, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
